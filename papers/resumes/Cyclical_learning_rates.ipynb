{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclical learning rate to train neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every neural network is trained through a gradient descent algorithm, where $\\theta_i^t = \\theta_i^{t-1} - \\alpha_{LR} \\cdot \\frac{\\partial L}{\\partial \\theta_i}$. $L$ is the loss function, $\\theta$ is the vector of parameters of the model, $\\alpha_{LR}$ is the learning rate. Commonly the learning rate should decreale monotonically during training, but there are some other methods to retrieve an optimal learning rate, such as exponential-brute-force searching or logarithmic decay. This paper shows that there exist a cyclical trend in the learning rate between its lower bound and its upper bound. In this region we can train the model with fewer iterations, **without a trade off on the accuracy of the model**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLR is related to adaptive learning rate, but it is a strong emprovement over this approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Mainly used approaches:\n",
    "- Increasing learning rate, immediate negative effects, but long time benefctial effects,\n",
    "- Exponentially decreasing learning rate,\n",
    "- Fixed learning rate. \n",
    "\n",
    "New approach: <u> let the learning rate vary through a range, firstly linear increasing then linear decreasing between the lower bound and the upper bound in a **triangular shape**</u>\n",
    "\n",
    "The reasons why it works:\n",
    "- Is mainly related to the behaviour of the loss function, because saddle points or partial minima points slows down the procedure\n",
    "- We can also assert that the optimal learning rate is between the bounds, and we train the best when around the optimal step. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________Pseudo-code sketch for programming the ciclycal behaviour________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l o c a l c y c l e = m a t h . f l o o r ( 1 +e p o c h C o u n t e r / ( 2 ∗ s t e p s i z e ) )\n",
    "\n",
    "l o c a l x = m a t h . a b s ( e p o c h C o u n t e r / s t e p s i z e − 2∗c y c l e + 1 )\n",
    "\n",
    "l o c a l l r = o p t . LR + ( maxLR − o p t . LR )∗ m a t h . max ( 0 , (1−x ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> MAIN CHALLENGES </u> *evaluate a correct value for the **cycle length*** : a good value is 2 $\\sim$ 10 times the number of iterations in an epoch (number of iterations required to fit all the trainset within the batchsize)\n",
    "\n",
    "<u> MAIN CHALLENGES </u> *evaluate reasonable **upper and lower bound***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________Algorithmic sketch to determinate a reasonable range for the learning rate________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"LR range test\":\n",
    "\n",
    "1) Run the model for many epochs while the learning rate runs through high LR through MIN_LR = 0 and MAX_LR = M\n",
    "2) Plot the accuracy with respect to the learing rate\n",
    "\n",
    "3a) Choose two values: LR1 is the value with respect to which the accuracy starts to increase, LR2 is the value with respect o which the accuracy start to behave irregularly\n",
    "    \n",
    "----> Assign MIN_LR = LR1 and MAX_LR = LR2\n",
    "\n",
    "3b) The optimal $LR^*$ can approximately be half or double the size the largest one that converges (?)\n",
    "    \n",
    "----> Assign $MIN_{LR} = \\frac{1}{3} LR^*, MAX_{LR} = LR^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cdd751ff1a78921eff5cc54f687db2b577836ff9e523662c7eedaeda872ff5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
